@INPROCEEDINGS{Yang:02:MBHPTWS, 
author={Ruigang Yang and Zhengyou Zhang}, 
booktitle={Automatic Face and Gesture Recognition, 2002. Proceedings. Fifth IEEE International Conference on}, 
title={Model-based head pose tracking with stereovision}, 
year={2002}, 
month={may}, 
volume={}, 
number={}, 
pages={255 -260}, 
abstract={We present a robust model-based stereo head tracking algorithm that operates in real time on a commodity PC. The use of an individualized three-dimensional head model, coupled with the epipolar constraint from the stereo image pair greatly improves the robustness of the tracking. Experimental results have shown that our method is able to track all the six degrees of freedom of the rigid part of head motions, over an extended period of time, in the presence of large angular and translational head motions, partial occlusions, and/or dramatic facial expression changes. Applications include human-computer interaction and eye-gaze correction for videoconferencing.}, 
keywords={commodity PC;dramatic facial expression changes;epipolar constraint;experimental results;eye-gaze correction;head motions;human-computer interaction;model-based head pose tracking;model-based stereo head tracking;partial occlusions;real time;six degrees of freedom;stereovision;three-dimensional head model;videoconferencing;face recognition;image motion analysis;real-time systems;stereo image processing;tracking;user interfaces;}, 
doi={10.1109/AFGR.2002.1004163}, 
ISSN={},}

@INPROCEEDINGS{Morency:03:PEU3VBE, 
author={Morency, L.-P. and Sundberg, P. and Darrell, T.}, 
booktitle={Analysis and Modeling of Faces and Gestures, 2003. AMFG 2003. IEEE International Workshop on}, 
title={Pose estimation using 3D view-based eigenspaces}, 
year={2003}, 
month={oct.}, 
volume={}, 
number={}, 
pages={ 45 - 52}, 
abstract={ We present a method for estimating the absolute pose of a rigid object based on intensity and depth view-based eigenspaces, built across multiple views of example objects of the same class. Given an initial frame of an object with unknown pose, we reconstruct a prior model for all views represented in the eigenspaces. For each new frame, we compute the pose-changes between every view of the reconstructed prior model and the new frame. The resulting pose-changes are then combined and used in a Kalman filter update. This approach for pose estimation is user-independent and the prior model can be initialized automatically from any viewpoint of the view-based eigenspaces. To track more robustly over time, we present an extension of this pose estimation technique where we integrate our prior model approach with an adaptive differential tracker. We demonstrate the accuracy of our approach on face pose tracking using stereo cameras.}, 
keywords={ 3D view-based eigenspaces; Kalman filter; adaptive differential tracker; face pose tracking; image reconstruction; pose estimation; principal component analysis; stereo cameras; Kalman filters; face recognition; image reconstruction; principal component analysis; solid modelling; tracking;}, 
doi={10.1109/AMFG.2003.1240823}, 
ISSN={},}

@INPROCEEDINGS{Breitenstein:08:RTFPEFSRI, 
author={Breitenstein, M.D. and Kuettel, D. and Weise, T. and van Gool, L. and Pfister, H.}, 
booktitle={Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on}, 
title={Real-time face pose estimation from single range images}, 
year={2008}, 
month={june}, 
volume={}, 
number={}, 
pages={1 -8}, 
abstract={We present a real-time algorithm to estimate the 3D pose of a previously unseen face from a single range image. Based on a novel shape signature to identify noses in range images, we generate candidates for their positions, and then generate and evaluate many pose hypotheses in parallel using modern graphics processing units (GPUs). We developed a novel error function that compares the input range image to precomputed pose images of an average face model. The algorithm is robust to large pose variations of plusmn90deg yaw, plusmn45deg pitch and plusmn30deg roll rotation, facial expression, partial occlusion, and works for multiple faces in the field of view. It correctly estimates 97.8% of the poses within yaw and pitch error of 15deg at 55.8 fps. To evaluate the algorithm, we built a database of range images with large pose variations and developed a method for automatic ground truth annotation.}, 
keywords={3D pose estimation;automatic ground truth annotation;average face model;facial expression;graphics processing unit;partial occlusion;pose variation;range image;real-time algorithm;real-time face pose estimation;shape signature;computer graphics;face recognition;pose estimation;real-time systems;}, 
doi={10.1109/CVPR.2008.4587807}, 
ISSN={1063-6919},}

@INPROCEEDINGS{Balasubramanian:07:BMEAFFPIHPE, 
author={Balasubramanian, V.N. and Jieping Ye and Panchanathan, S.}, 
booktitle={Computer Vision and Pattern Recognition, 2007. CVPR '07. IEEE Conference on}, 
title={Biased Manifold Embedding: A Framework for Person-Independent Head Pose Estimation}, 
year={2007}, 
month={june}, 
volume={}, 
number={}, 
pages={1 -7}, 
abstract={The estimation of head pose angle from face images is an integral component of face recognition systems, human computer interfaces and other human-centered computing applications. To determine the head pose, face images with varying pose angles can be considered to be lying on a smooth low-dimensional manifold in high-dimensional feature space. While manifold learning techniques capture the geometrical relationship between data points in the high-dimensional image feature space, the pose label information of the training data samples are neglected in the computation of these embeddings. In this paper, we propose a novel supervised approach to manifold-based non-linear dimensionality reduction for head pose estimation. The Biased Manifold Embedding (BME) framework is pivoted on the ideology of using the pose angle information of the face images to compute a biased neighborhood of each point in the feature space, before determining the low-dimensional embedding. The proposed BME approach is formulated as an extensible framework, and validated with the Isomap, Locally Linear Embedding (LLE) and Laplacian Eigen-maps techniques. A Generalized Regression Neural Network (GRNN) is used to learn the non-linear mapping, and linear multi-variate regression is finally applied on the low-dimensional space to obtain the pose angle. We tested this approach on face images of 24 individuals with pose angles varying from -90deg to +90deg with a granularity of 2. The results showed substantial reduction in the error of pose angle estimation, and robustness to variations in feature spaces, dimensionality of embedding and other parameters.}, 
keywords={Laplacian eigen-maps techniques;biased manifold embedding;face images;face recognition systems;generalized regression neural network;high-dimensional feature space;linear multivariate regression;locally linear embedding;manifold learning techniques;manifold-based nonlinear dimensionality;person-independent head pose estimation;pose label information;Laplace equations;eigenvalues and eigenfunctions;face recognition;feature extraction;learning (artificial intelligence);neural nets;regression analysis;}, 
doi={10.1109/CVPR.2007.383280}, 
ISSN={},}

@article{Osadchy:07:SFDPEEBM,
author = {Osadchy, Margarita and Cun, Yann Le and Miller, Matthew L.},
title = {Synergistic Face Detection and Pose Estimation with Energy-Based Models},
journal = {J. Mach. Learn. Res.},
volume = {8},
month = {May},
year = {2007},
issn = {1532-4435},
pages = {1197--1215},
numpages = {19},
url = {http://dl.acm.org/citation.cfm?id=1248659.1248700},
acmid = {1248700},
publisher = {JMLR.org},
abstract = {We describe a novel method for simultaneously detecting faces and estimating their pose in real time. The method employs a convolutional network to map images of faces to points on a low-dimensional manifold parametrized by pose, and images of non-faces to points far away from that manifold. Given an image, detecting a face and estimating its pose is viewed as minimizing an energy function with respect to the face/non-face binary variable and the continuous pose parameters. The system is trained to minimize a loss function that drives correct combinations of labels and pose to be associated with lower energy values than incorrect ones. The system is designed to handle very large range of poses without retraining. The performance of the system was tested on three standard data sets---for frontal views, rotated faces, and profiles---is comparable to previous systems that are designed to handle a single one of these data sets. We show that a system trained simuiltaneously for detection and pose estimation is more accurate on both tasks than similar systems trained for each task separately.},}

@INPROCEEDINGS{Storer:09:3DMAM, 
author={Storer, M. and Urschler, M. and Bischof, H.}, 
booktitle={Computer Vision Workshops (ICCV Workshops), 2009 IEEE 12th International Conference on}, 
title={3D-MAM: 3D morphable appearance model for efficient fine head pose estimation from still images}, 
year={2009}, 
month={27 2009-oct. 4}, 
volume={}, 
number={}, 
pages={192 -199}, 
abstract={Identity-invariant estimation of head pose from still images is a challenging task due to the high variability of facial appearance. We present a novel 3D head pose estimation approach, which utilizes the flexibility and expressibility of a dense generative 3D facial model in combination with a very fast fitting algorithm. The efficiency of the head pose estimation is obtained by a 2D synthesis of the facial input image. This optimization procedure drives the appearance and pose of the 3D facial model. In contrast to many other approaches we are specifically interested in the more difficult task of head pose estimation from still images, instead of tracking faces in image sequences. We evaluate our approach on two publicly available databases (FacePix and USF HumanID) and compare our method to the 3D morphable model and other state of the art approaches in terms of accuracy and speed.}, 
keywords={2D synthesis;3D head pose estimation approach;3D morphable appearance model;HumanID;facial appearance;fitting algorithm;identity-invariant estimation;face recognition;optimisation;pose estimation;}, 
doi={10.1109/ICCVW.2009.5457701}, 
ISSN={},}

@INPROCEEDINGS{Vatahska:07:FBHPEFI, 
author={Vatahska, T. and Bennewitz, M. and Behnke, S.}, 
booktitle={Humanoid Robots, 2007 7th IEEE-RAS International Conference on}, 
title={Feature-based head pose estimation from images}, 
year={2007}, 
month={29 2007-dec. 1}, 
volume={}, 
number={}, 
pages={330 -335}, 
abstract={Estimating the head pose is an important capability of a robot when interacting with humans since the head pose usually indicates the focus of attention. In this paper, we present a novel approach to estimate the head pose from monocular images. Our approach proceeds in three stages. First, a face detector roughly classifies the pose as frontal, left, or right profile. Then, classifiers trained with AdaBoost using Haar-like features, detect distinctive facial features such as the nose tip and the eyes. Based on the positions of these features, a neural network finally estimates the three continuous rotation angles we use to model the head pose. Since we have a compact representation of the face using only few distinctive features, our approach is computationally highly efficient. As we show in experiments with standard databases as well as with real-time image data, our system locates the distinctive features with a high accuracy and provides robust estimates of the head pose.}, 
keywords={AdaBoost algorithm;Haar-like feature-based robot head pose estimation;continuous rotation angle estimation;face detection;left profile;monocular image classification training;neural network;right profile;feature extraction;image classification;intelligent robots;learning (artificial intelligence);neurocontrollers;pose estimation;robot vision;}, 
doi={10.1109/ICHR.2007.4813889}, 
ISSN={},}

@article{Malassiotis:05:RRHPEFRD,
 author = {Malassiotis, Sotiris and Strintzis, Michael G.},
 title = {Robust real-time 3D head pose estimation from range data},
 journal = {Pattern Recogn.},
 volume = {38},
 issue = {8},
 month = {August},
 year = {2005},
 issn = {0031-3203},
 pages = {1153--1165},
 numpages = {13},
 url = {http://dx.doi.org/10.1016/j.patcog.2004.11.020},
 doi = {http://dx.doi.org/10.1016/j.patcog.2004.11.020},
 acmid = {1746581},
 publisher = {Elsevier Science Inc.},
 address = {New York, NY, USA},
 keywords = {Face, Head, Pose, Range data, Tracking},
 abstract = {In this paper a real-time 3D pose estimation algorithm using range data is described. The system relies on a novel 3D sensor that generates a dense range image of the scene. By not relying on brightness information, the proposed system guarantees robustness under a variety of illumination conditions, and scene contents. Efficient face detection using global features and exploitation of prior knowledge along with novel feature localization and tracking techniques are described. Experimental results demonstrate accurate estimation of the six degrees of freedom of the head and robustness under occlusions, facial expressions, and head shape variability.}
}

@INPROCEEDINGS{Matsumoto:00:AAFRTSVIOHPAGDM, 
author={Matsumoto, Y. and Zelinsky, A.}, 
booktitle={Automatic Face and Gesture Recognition, 2000. Proceedings. Fourth IEEE International Conference on}, 
title={An algorithm for real-time stereo vision implementation of head pose and gaze direction measurement}, 
year={2000}, 
month={}, 
volume={}, 
number={}, 
pages={499 -504}, 
abstract={To build smart human interfaces, it is necessary for a system to know a user's intention and point of attention. Since the motion of a person's head pose and gaze direction are deeply related with his/her intention and attention, detection of such information can be utilized to build natural and intuitive interfaces. We describe our real-time stereo face tracking and gaze detection system to measure head pose and gaze direction simultaneously. The key aspect of our system is the use of real-time stereo vision together with a simple algorithm which is suitable for real-time processing. Since the 3D coordinates of the features on a face can be directly measured in our system, we can significantly simplify the algorithm for 3D model fitting to obtain the full 3D pose of the head compared with conventional systems that use monocular camera. Consequently we achieved a non-contact, passive, real-time, robust, accurate and compact measurement system for head pose and gaze direction}, 
keywords={3D feature coordinates;3D model fitting;face tracking;gaze direction measurement;head motion;head pose;measurement system;natural intuitive interfaces;real-time stereo vision;smart human interfaces;face recognition;feature extraction;gesture recognition;real-time systems;stereo image processing;tracking;user interface management systems;}, 
doi={10.1109/AFGR.2000.840680}, 
ISSN={},}

@INPROCEEDINGS{Yao:04:EMBLHMRFM, 
author={Jian Yao and Wai-Kuen Cham}, 
booktitle={Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on}, 
title={Efficient model-based linear head motion recovery from movies}, 
year={2004}, 
month={june-2 july}, 
volume={2}, 
number={}, 
pages={ II-414 - II-421 Vol.2}, 
abstract={ We propose an efficient method that estimates the motion parameters of a human head from a video sequence by using a three-layer linear iterative process. In the innermost layer, we estimate the motion of each input face image in a video sequence based on a generic face model and a small set of feature points. A fast iterative least-square method is used to recover these motion parameters. After that, we iteratively estimate three model scaling factors using multiple frames with the recovered poses in the middle layer. Finally, we update 3D coordinates of the feature points on the generic face model in the outer-most layer. Since all iterative processes can be solved linearly, the computational cost is low. Tests on synthetic data under noisy conditions and two real video sequences have been performed. Experimental results show that the proposed method is robust and has good performance.}, 
keywords={ feature points; iterative least-square method; linear head motion recovery; movies; three-layer linear iterative process; video sequence; feature extraction; image denoising; image sequences; iterative methods; least squares approximations; motion estimation;}, 
doi={10.1109/CVPR.2004.1315193}, 
ISSN={1063-6919 },}

@INPROCEEDINGS{Weise:07:F3SWAMC, 
author={Weise, T. and Leibe, B. and Van Gool, L.}, 
booktitle={Computer Vision and Pattern Recognition, 2007. CVPR '07. IEEE Conference on}, 
title={Fast 3D Scanning with Automatic Motion Compensation}, 
year={2007}, 
month={june}, 
volume={}, 
number={}, 
pages={1 -8}, 
abstract={We present a novel 3D scanning system combining stereo and active illumination based on phase-shift for robust and accurate scene reconstruction. Stereo overcomes the traditional phase discontinuity problem and allows for the reconstruction of complex scenes containing multiple objects. Due to the sequential recording of three patterns, motion will introduce artifacts in the reconstruction. We develop a closed-form expression for the motion error in order to apply motion compensation on a pixel level. The resulting scanning system can capture accurate depth maps of complex dynamic scenes at 17 fps and can cope with both rigid and deformable objects.}, 
keywords={3D scanning system;active illumination;automatic motion compensation;phase discontinuity problem;pixel level;scene reconstruction;stereo illumination;image resolution;image scanners;lighting;motion compensation;}, 
doi={10.1109/CVPR.2007.383291}, 
ISSN={},}

@inproceedings{Cai:2010:DFTWACDC,
 author = {Cai, Qin and Gallup, David and Zhang, Cha and Zhang, Zhengyou},
 title = {3D deformable face tracking with a commodity depth camera},
 booktitle = {Proceedings of the 11th European conference on computer vision conference on Computer vision: Part III},
 series = {ECCV'10},
 year = {2010},
 isbn = {3-642-15557-X, 978-3-642-15557-4},
 location = {Heraklion, Crete, Greece},
 pages = {229--242},
 numpages = {14},
 url = {http://dl.acm.org/citation.cfm?id=1927006.1927026},
 acmid = {1927026},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
}

@ARTICLE{Murphy:09:SURVEY, 
author={Murphy-Chutorian, E. and Trivedi, M.M.}, 
journal={Pattern Analysis and Machine Intelligence, IEEE Transactions on}, 
title={Head Pose Estimation in Computer Vision: A Survey}, 
year={2009}, 
month={april }, 
volume={31}, 
number={4}, 
pages={607 -626}, 
abstract={The capacity to estimate the head pose of another person is a common human ability that presents a unique challenge for computer vision systems. Compared to face detection and recognition, which have been the primary foci of face-related vision research, identity-invariant head pose estimation has fewer rigorously evaluated systems or generic solutions. In this paper, we discuss the inherent difficulties in head pose estimation and present an organized survey describing the evolution of the field. Our discussion focuses on the advantages and disadvantages of each approach and spans 90 of the most innovative and characteristic papers that have been published on this topic. We compare these systems by focusing on their ability to estimate coarse and fine head pose, highlighting approaches that are well suited for unconstrained environments.}, 
keywords={computer vision;face detection;generic solution;human ability;image recognition;pose estimation;computer vision;pose estimation;Artificial Intelligence;Head;Humans;Nonlinear Dynamics;Video Recording;Visual Perception;}, 
doi={10.1109/TPAMI.2008.106}, 
ISSN={0162-8828},}

@INPROCEEDINGS{Fanelli:11:RTHPEWRRF, 
author={Fanelli, G. and Gall, J. and Van Gool, L.}, 
booktitle={Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on}, 
title={Real time head pose estimation with random regression forests}, 
year={2011}, 
month={june}, 
volume={}, 
number={}, 
pages={617 -624}, 
abstract={Fast and reliable algorithms for estimating the head pose are essential for many applications and higher-level face analysis tasks. We address the problem of head pose estimation from depth data, which can be captured using the ever more affordable 3D sensing technologies available today. To achieve robustness, we formulate pose estimation as a regression problem. While detecting specific face parts like the nose is sensitive to occlusions, learning the regression on rather generic surface patches requires enormous amount of training data in order to achieve accurate estimates. We propose to use random regression forests for the task at hand, given their capability to handle large training datasets. Moreover, we synthesize a great amount of annotated training data using a statistical model of the human face. In our experiments, we show that our approach can handle real data presenting large pose changes, partial occlusions, and facial expressions, even though it is trained only on synthetic neutral face data. We have thoroughly evaluated our system on a publicly available database on which we achieve state-of-the-art performance without having to resort to the graphics card.}, 
keywords={3D sensing technologies;face analysis tasks;facial expressions;graphics card;human face;large training datasets;partial occlusions;random regression forests;real time head pose estimation;specific face detection;statistical model;synthetic neutral face data;face recognition;object detection;pose estimation;regression analysis;}, 
doi={10.1109/CVPR.2011.5995458}, 
ISSN={1063-6919},}

@INPROCEEDINGS{Chen:03:HPEUFML, 
author={Chen, L. and Zhang, L. and Hu, Y. and Li, M. and Zhang, H.}, 
booktitle={Analysis and Modeling of Faces and Gestures, 2003. AMFG 2003. IEEE International Workshop on}, 
title={Head pose estimation using Fisher Manifold learning}, 
year={2003}, 
month={oct.}, 
volume={}, 
number={}, 
pages={ 203 - 207}, 
abstract={Here, we propose a new learning strategy for head pose estimation. Our approach uses nonlinear interpolation to estimate the head pose using the learning result from face images of two head poses. Advantage of our method to regression method is that it only requires training images of two head poses and better generalization ability. It outperforms existed methods, such as regression and multiclass classification method, on both synthesis and real face images. Average head pose estimation error of yaw rotation is about 40, which proves that our method is effective in head pose estimation.}, 
keywords={ Fisher Manifold learning; face images; head pose estimation; multiclass classification method; nonlinear interpolation; regression method; face recognition; image classification; interpolation; learning (artificial intelligence); regression analysis; support vector machines;}, 
doi={10.1109/AMFG.2003.1240844}, 
ISSN={},}

@INPROCEEDINGS{Whitehill:08:ADATFBFHPT, 
author={Whitehill, J. and Movellan, J.R.}, 
booktitle={Automatic Face Gesture Recognition, 2008. FG '08. 8th IEEE International Conference on}, 
title={A discriminative approach to frame-by-frame head pose tracking}, 
year={2008}, 
month={sept.}, 
volume={}, 
number={}, 
pages={1 -7}, 
abstract={We present a discriminative approach to frame-by-frame head pose tracking that is robust to a wide range of illuminations and facial appearances and that is inherently immune to accuracy drift. Most previous research on head pose tracking has been validated on test datasets spanning only a small ( lt; 20) subjects under controlled illumination conditions on continuous video sequences. In contrast, the system presented in this paper was both trained and tested on a much larger database, GENKI, spanning tens of thousands of different subjects, illuminations, and geographical locations from images on the Web. Our pose estimator achieves accuracy of 5.82deg, 5.65deg, and 2.96deg root-mean-square (RMS) error for yaw, pitch, and roll, respectively. A set of 4000 images from this dataset, labeled for pose, was collected and released for use by the research community.}, 
keywords={accuracy drift;continuous video sequence;controlled illumination condition;discriminative approach;facial appearance;frame-by-frame head pose tracking;pose estimation;root-mean-square error;face recognition;mean square error methods;pose estimation;tracking;}, 
doi={10.1109/AFGR.2008.4813396}, 
ISSN={},}

@article{Weise:11:RPBFA,
 author = {Weise, Thibaut and Bouaziz, Sofien and Li, Hao and Pauly, Mark},
 title = {Realtime performance-based facial animation},
 journal = {ACM Trans. Graph.},
 issue_date = {July 2011},
 volume = {30},
 number = {4},
 month = aug,
 year = {2011},
 issn = {0730-0301},
 pages = {77:1--77:10},
 articleno = {77},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2010324.1964972},
 doi = {10.1145/2010324.1964972},
 acmid = {1964972},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {blendshape animation, face animation, markerless performance capture, real-time tracking},
}

@INPROCEEDINGS{Seemann:04:HPEUSVFHRI, 
author={Seemann, E. and Nickel, K. and Stiefelhagen, R.}, 
booktitle={Automatic Face and Gesture Recognition, 2004. Proceedings. Sixth IEEE International Conference on}, 
title={Head pose estimation using stereo vision for human-robot interaction}, 
year={2004}, 
month={may}, 
volume={}, 
number={}, 
pages={ 626 - 631}, 
abstract={We present a method for estimating a person's head pose with a stereo camera. Our approach focuses on the application of human-robot interaction, where people may be further away from the camera and move freely around in a room. We show that depth information acquired from a stereo camera not only helps improving the accuracy of the pose estimation, but also improves the robustness of the system when the lighting conditions change. The estimation is based on neural networks, which are trained to compute the head pose from grayscale and disparity images of the stereo camera. It can handle pan and tilt rotations from -90 deg; to +90 deg;. Our system does not require any manual initialization and does not suffer from drift during an image sequence. Moreover the system is capable of real-time processing.}, 
keywords={ disparity images; grayscale images; head pose estimation; human-robot interaction; neural networks; stereo camera; stereo vision; gesture recognition; human computer interaction; image motion analysis; neural nets; stereo image processing;}, 
doi={10.1109/AFGR.2004.1301603}, 
ISSN={ },}


@Patent{Yang:99:USP,
 author = {Tzong-Jer Yang, Fu-Che Wu, Ming Ouhyoung},
 title = {Method of image processing using three facial feature points in three-dimensional head motion tracking},
 year = {2003},
 month = {06},
 day = {17},
 number = {US 6580810},
 type = {Patent},
 location = {US},
 url = {http://www.patentlens.net/patentlens/patent/US_6580810/},
 filing_num = {09329671},
 yearfiled = {1999},
 monthfiled = {06},
 dayfiled = {10},
 IPC_class = {G06K 900; H04N 714;},
 US_class = {348/14.1; 382/103; 382/154},
 abstract = {A method of image processing in three-dimensional (3-D) head motion tracking is disclosed. The method includes: providing a user's source image to a first processing device; capturing the user's first image and providing it to a second processing device; selecting three facial feature points of the first image from the second processing device to form a 3-D feature triangle; capturing user's consecutive video frames and providing them to the second processing device when the user proceeds with head motions; tracking the three facial feature points corresponding to the consecutive video frames to form a series of actual 2-D feature triangle; rotating and translating the 3-D feature triangle freely to form a plurality of geometric transformations, selecting one of the geometric transformations with acceptable error between the two consecutive 2-D feature triangles, repeating the step until the last frame of the consecutive video frames and geometric transformations corresponding to various consecutive video frames are formed; and providing the geometric transformations to the first processing device to generate a head motion corresponding to the user's source image.},
}
