@INPROCEEDINGS{Yang:02:MBHPTWS, 
author={Ruigang Yang and Zhengyou Zhang}, 
booktitle={Automatic Face and Gesture Recognition, 2002. Proceedings. Fifth IEEE International Conference on}, 
title={Model-based head pose tracking with stereovision}, 
year={2002}, 
month={may}, 
volume={}, 
number={}, 
pages={255 -260}, 
abstract={We present a robust model-based stereo head tracking algorithm that operates in real time on a commodity PC. The use of an individualized three-dimensional head model, coupled with the epipolar constraint from the stereo image pair greatly improves the robustness of the tracking. Experimental results have shown that our method is able to track all the six degrees of freedom of the rigid part of head motions, over an extended period of time, in the presence of large angular and translational head motions, partial occlusions, and/or dramatic facial expression changes. Applications include human-computer interaction and eye-gaze correction for videoconferencing.}, 
keywords={commodity PC;dramatic facial expression changes;epipolar constraint;experimental results;eye-gaze correction;head motions;human-computer interaction;model-based head pose tracking;model-based stereo head tracking;partial occlusions;real time;six degrees of freedom;stereovision;three-dimensional head model;videoconferencing;face recognition;image motion analysis;real-time systems;stereo image processing;tracking;user interfaces;}, 
doi={10.1109/AFGR.2002.1004163}, 
ISSN={},}

@INPROCEEDINGS{Morency:03:PEU3VBE, 
author={Morency, L.-P. and Sundberg, P. and Darrell, T.}, 
booktitle={Analysis and Modeling of Faces and Gestures, 2003. AMFG 2003. IEEE International Workshop on}, 
title={Pose estimation using 3D view-based eigenspaces}, 
year={2003}, 
month={oct.}, 
volume={}, 
number={}, 
pages={ 45 - 52}, 
abstract={ We present a method for estimating the absolute pose of a rigid object based on intensity and depth view-based eigenspaces, built across multiple views of example objects of the same class. Given an initial frame of an object with unknown pose, we reconstruct a prior model for all views represented in the eigenspaces. For each new frame, we compute the pose-changes between every view of the reconstructed prior model and the new frame. The resulting pose-changes are then combined and used in a Kalman filter update. This approach for pose estimation is user-independent and the prior model can be initialized automatically from any viewpoint of the view-based eigenspaces. To track more robustly over time, we present an extension of this pose estimation technique where we integrate our prior model approach with an adaptive differential tracker. We demonstrate the accuracy of our approach on face pose tracking using stereo cameras.}, 
keywords={ 3D view-based eigenspaces; Kalman filter; adaptive differential tracker; face pose tracking; image reconstruction; pose estimation; principal component analysis; stereo cameras; Kalman filters; face recognition; image reconstruction; principal component analysis; solid modelling; tracking;}, 
doi={10.1109/AMFG.2003.1240823}, 
ISSN={},}

@INPROCEEDINGS{Breitenstein:08:RTFPEFSRI, 
author={Breitenstein, M.D. and Kuettel, D. and Weise, T. and van Gool, L. and Pfister, H.}, 
booktitle={Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on}, 
title={Real-time face pose estimation from single range images}, 
year={2008}, 
month={june}, 
volume={}, 
number={}, 
pages={1 -8}, 
abstract={We present a real-time algorithm to estimate the 3D pose of a previously unseen face from a single range image. Based on a novel shape signature to identify noses in range images, we generate candidates for their positions, and then generate and evaluate many pose hypotheses in parallel using modern graphics processing units (GPUs). We developed a novel error function that compares the input range image to precomputed pose images of an average face model. The algorithm is robust to large pose variations of plusmn90deg yaw, plusmn45deg pitch and plusmn30deg roll rotation, facial expression, partial occlusion, and works for multiple faces in the field of view. It correctly estimates 97.8% of the poses within yaw and pitch error of 15deg at 55.8 fps. To evaluate the algorithm, we built a database of range images with large pose variations and developed a method for automatic ground truth annotation.}, 
keywords={3D pose estimation;automatic ground truth annotation;average face model;facial expression;graphics processing unit;partial occlusion;pose variation;range image;real-time algorithm;real-time face pose estimation;shape signature;computer graphics;face recognition;pose estimation;real-time systems;}, 
doi={10.1109/CVPR.2008.4587807}, 
ISSN={1063-6919},}

@INPROCEEDINGS{Balasubramanian:07:BMEAFFPIHPE, 
author={Balasubramanian, V.N. and Jieping Ye and Panchanathan, S.}, 
booktitle={Computer Vision and Pattern Recognition, 2007. CVPR '07. IEEE Conference on}, 
title={Biased Manifold Embedding: A Framework for Person-Independent Head Pose Estimation}, 
year={2007}, 
month={june}, 
volume={}, 
number={}, 
pages={1 -7}, 
abstract={The estimation of head pose angle from face images is an integral component of face recognition systems, human computer interfaces and other human-centered computing applications. To determine the head pose, face images with varying pose angles can be considered to be lying on a smooth low-dimensional manifold in high-dimensional feature space. While manifold learning techniques capture the geometrical relationship between data points in the high-dimensional image feature space, the pose label information of the training data samples are neglected in the computation of these embeddings. In this paper, we propose a novel supervised approach to manifold-based non-linear dimensionality reduction for head pose estimation. The Biased Manifold Embedding (BME) framework is pivoted on the ideology of using the pose angle information of the face images to compute a biased neighborhood of each point in the feature space, before determining the low-dimensional embedding. The proposed BME approach is formulated as an extensible framework, and validated with the Isomap, Locally Linear Embedding (LLE) and Laplacian Eigen-maps techniques. A Generalized Regression Neural Network (GRNN) is used to learn the non-linear mapping, and linear multi-variate regression is finally applied on the low-dimensional space to obtain the pose angle. We tested this approach on face images of 24 individuals with pose angles varying from -90deg to +90deg with a granularity of 2. The results showed substantial reduction in the error of pose angle estimation, and robustness to variations in feature spaces, dimensionality of embedding and other parameters.}, 
keywords={Laplacian eigen-maps techniques;biased manifold embedding;face images;face recognition systems;generalized regression neural network;high-dimensional feature space;linear multivariate regression;locally linear embedding;manifold learning techniques;manifold-based nonlinear dimensionality;person-independent head pose estimation;pose label information;Laplace equations;eigenvalues and eigenfunctions;face recognition;feature extraction;learning (artificial intelligence);neural nets;regression analysis;}, 
doi={10.1109/CVPR.2007.383280}, 
ISSN={},}

@article{Osadchy:07:SFDPEEBM,
author = {Osadchy, Margarita and Cun, Yann Le and Miller, Matthew L.},
title = {Synergistic Face Detection and Pose Estimation with Energy-Based Models},
journal = {J. Mach. Learn. Res.},
volume = {8},
month = {May},
year = {2007},
issn = {1532-4435},
pages = {1197--1215},
numpages = {19},
url = {http://dl.acm.org/citation.cfm?id=1248659.1248700},
acmid = {1248700},
publisher = {JMLR.org},
abstract = {We describe a novel method for simultaneously detecting faces and estimating their pose in real time. The method employs a convolutional network to map images of faces to points on a low-dimensional manifold parametrized by pose, and images of non-faces to points far away from that manifold. Given an image, detecting a face and estimating its pose is viewed as minimizing an energy function with respect to the face/non-face binary variable and the continuous pose parameters. The system is trained to minimize a loss function that drives correct combinations of labels and pose to be associated with lower energy values than incorrect ones. The system is designed to handle very large range of poses without retraining. The performance of the system was tested on three standard data sets---for frontal views, rotated faces, and profiles---is comparable to previous systems that are designed to handle a single one of these data sets. We show that a system trained simuiltaneously for detection and pose estimation is more accurate on both tasks than similar systems trained for each task separately.},}

@INPROCEEDINGS{Storer:09:3DMAM, 
author={Storer, M. and Urschler, M. and Bischof, H.}, 
booktitle={Computer Vision Workshops (ICCV Workshops), 2009 IEEE 12th International Conference on}, 
title={3D-MAM: 3D morphable appearance model for efficient fine head pose estimation from still images}, 
year={2009}, 
month={27 2009-oct. 4}, 
volume={}, 
number={}, 
pages={192 -199}, 
abstract={Identity-invariant estimation of head pose from still images is a challenging task due to the high variability of facial appearance. We present a novel 3D head pose estimation approach, which utilizes the flexibility and expressibility of a dense generative 3D facial model in combination with a very fast fitting algorithm. The efficiency of the head pose estimation is obtained by a 2D synthesis of the facial input image. This optimization procedure drives the appearance and pose of the 3D facial model. In contrast to many other approaches we are specifically interested in the more difficult task of head pose estimation from still images, instead of tracking faces in image sequences. We evaluate our approach on two publicly available databases (FacePix and USF HumanID) and compare our method to the 3D morphable model and other state of the art approaches in terms of accuracy and speed.}, 
keywords={2D synthesis;3D head pose estimation approach;3D morphable appearance model;HumanID;facial appearance;fitting algorithm;identity-invariant estimation;face recognition;optimisation;pose estimation;}, 
doi={10.1109/ICCVW.2009.5457701}, 
ISSN={},}

@INPROCEEDINGS{Vatahska:07:FBHPEFI, 
author={Vatahska, T. and Bennewitz, M. and Behnke, S.}, 
booktitle={Humanoid Robots, 2007 7th IEEE-RAS International Conference on}, 
title={Feature-based head pose estimation from images}, 
year={2007}, 
month={29 2007-dec. 1}, 
volume={}, 
number={}, 
pages={330 -335}, 
abstract={Estimating the head pose is an important capability of a robot when interacting with humans since the head pose usually indicates the focus of attention. In this paper, we present a novel approach to estimate the head pose from monocular images. Our approach proceeds in three stages. First, a face detector roughly classifies the pose as frontal, left, or right profile. Then, classifiers trained with AdaBoost using Haar-like features, detect distinctive facial features such as the nose tip and the eyes. Based on the positions of these features, a neural network finally estimates the three continuous rotation angles we use to model the head pose. Since we have a compact representation of the face using only few distinctive features, our approach is computationally highly efficient. As we show in experiments with standard databases as well as with real-time image data, our system locates the distinctive features with a high accuracy and provides robust estimates of the head pose.}, 
keywords={AdaBoost algorithm;Haar-like feature-based robot head pose estimation;continuous rotation angle estimation;face detection;left profile;monocular image classification training;neural network;right profile;feature extraction;image classification;intelligent robots;learning (artificial intelligence);neurocontrollers;pose estimation;robot vision;}, 
doi={10.1109/ICHR.2007.4813889}, 
ISSN={},}

@article{Malassiotis:05:RRHPEFRD,
 author = {Malassiotis, Sotiris and Strintzis, Michael G.},
 title = {Robust real-time 3D head pose estimation from range data},
 journal = {Pattern Recogn.},
 volume = {38},
 issue = {8},
 month = {August},
 year = {2005},
 issn = {0031-3203},
 pages = {1153--1165},
 numpages = {13},
 url = {http://dx.doi.org/10.1016/j.patcog.2004.11.020},
 doi = {http://dx.doi.org/10.1016/j.patcog.2004.11.020},
 acmid = {1746581},
 publisher = {Elsevier Science Inc.},
 address = {New York, NY, USA},
 keywords = {Face, Head, Pose, Range data, Tracking},
 abstract = {In this paper a real-time 3D pose estimation algorithm using range data is described. The system relies on a novel 3D sensor that generates a dense range image of the scene. By not relying on brightness information, the proposed system guarantees robustness under a variety of illumination conditions, and scene contents. Efficient face detection using global features and exploitation of prior knowledge along with novel feature localization and tracking techniques are described. Experimental results demonstrate accurate estimation of the six degrees of freedom of the head and robustness under occlusions, facial expressions, and head shape variability.}
}